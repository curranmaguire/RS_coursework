{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating our enhanced model\n",
    "### enhancements\n",
    "- we need to model availability into our model \n",
    "- we need to model repeat consumption as users will repeatedly consume the days after but two days after less and 3 days after it is not as likely\n",
    "- we need to model users with high interactions and users with low interactions. Tow models?\n",
    "\n",
    "create a NCF\n",
    "test it\n",
    "create an availability measure. This could be doing a daily version where users are modelled by their day to day interactions? \n",
    "test model on upper and lower quartiles, see if there is a change in acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users:  100000\n",
      "Num streamers:  162625\n",
      "Num interactions:  3051733\n",
      "Num users:  1474\n",
      "Num streamers:  1380\n",
      "Num interactions:  100000\n",
      "sparsity is  95.08%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(),'Datasets/100k_a.csv')\n",
    "cols = [\"user\",\"stream\",\"streamer\",\"start\",\"stop\"]\n",
    "data = pd.read_csv(file_path, header=None, names=cols)\n",
    "data.user = pd.factorize(data.user)[0]+1\n",
    "data['streamer_raw'] = data.streamer\n",
    "data.streamer = pd.factorize(data.streamer)[0]+1\n",
    "print(\"Num users: \", data.user.nunique())\n",
    "print(\"Num streamers: \", data.streamer.nunique())\n",
    "print(\"Num interactions: \", len(data))\n",
    "\n",
    "def custom_train_test_split(interactions_matrix, test_size=0.2):\n",
    "    # Set initial training and testing matrices\n",
    "    train_matrix = np.zeros_like(interactions_matrix)\n",
    "    test_matrix = np.zeros_like(interactions_matrix)\n",
    "    \n",
    "    # Ensure at least one interaction per user and per streamer in the training set\n",
    "    for user in range(interactions_matrix.shape[0]):\n",
    "        user_interactions = np.where(interactions_matrix[user, :] > 0)[0]\n",
    "        if len(user_interactions) > 0:\n",
    "            selected = np.random.choice(user_interactions, size=1)\n",
    "            train_matrix[user, selected] = interactions_matrix[user, selected]\n",
    "\n",
    "    for streamer in range(interactions_matrix.shape[1]):\n",
    "        streamer_interactions = np.where(interactions_matrix[:, streamer] > 0)[0]\n",
    "        if len(streamer_interactions) > 0:\n",
    "            selected = np.random.choice(streamer_interactions, size=1)\n",
    "            train_matrix[selected, streamer] = interactions_matrix[selected, streamer]\n",
    "\n",
    "    # Distribute the rest of the interactions\n",
    "    remaining_indices = np.where((interactions_matrix != train_matrix) & (interactions_matrix > 0))\n",
    "    remaining_indices = list(zip(remaining_indices[0], remaining_indices[1]))\n",
    "    test_indices = np.random.choice(np.arange(len(remaining_indices)), size=int(len(remaining_indices) * test_size), replace=False)\n",
    "    test_indices = [remaining_indices[i] for i in test_indices]\n",
    "\n",
    "    for user, streamer in test_indices:\n",
    "        test_matrix[user, streamer] = interactions_matrix[user, streamer]\n",
    "        train_matrix[user, streamer] = 0\n",
    "    return train_matrix, test_matrix\n",
    "\n",
    "\n",
    "#let's train on the top 100,000 entries. These are the entries which decrease the sparsity the most.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#calculate the user incteraction frequency\n",
    "user_interactions = data.groupby('user').size()\n",
    "streamer_interactions = data.groupby('streamer').size()\n",
    "#rank the users and streamers by their frequency\n",
    "user_ranks = user_interactions.rank(method='first', ascending=False)\n",
    "streamer_ranks = streamer_interactions.rank(method='first', ascending=False)\n",
    "#score the interactions and select the top 100,000\n",
    "data['user_score'] = data['user'].map(user_ranks)\n",
    "data['streamer_score'] = data['streamer'].map(streamer_ranks)\n",
    "data['combined_score'] = data['user_score'] + data['streamer_score']\n",
    "data['rating'] = data.stop - data.start\n",
    "\n",
    "\n",
    "top_data = data.nsmallest(100000, 'combined_score')\n",
    "filtered_data = top_data.drop(columns=['user_score', 'streamer_score', 'combined_score'])\n",
    "\n",
    "print(\"Num users: \", filtered_data.user.nunique())\n",
    "print(\"Num streamers: \", filtered_data.streamer.nunique())\n",
    "print(\"Num interactions: \", len(filtered_data))\n",
    "\n",
    "# check for sparsity of the data\n",
    "potential_num_interactions = filtered_data.streamer.nunique() * filtered_data.user.nunique()\n",
    "num_interactions = len(filtered_data['streamer']) #each row in the filtered_data is an interaction\n",
    "sparsity = (1-num_interactions /potential_num_interactions) * 100\n",
    "print(f'sparsity is {sparsity: .2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1474, 1380) (1474, 1380) (1474, 1380)\n"
     ]
    }
   ],
   "source": [
    "#creating a user item interaction matrix\n",
    "user_streamer_interactions = filtered_data.pivot_table(index='user', columns='streamer', values='rating', aggfunc='sum', fill_value=0).values\n",
    "#normalize the items \n",
    "user_streamer_interactions = MinMaxScaler().fit_transform(user_streamer_interactions)\n",
    "\n",
    "#this makes sure that all the users and streamers are represented in the training and testing data\n",
    "train_interactions, test_interactions = custom_train_test_split(user_streamer_interactions, test_size=0.2)\n",
    "\n",
    "print(user_streamer_interactions.shape, test_interactions.shape, train_interactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-16eda89d9b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNCF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#items in this will be streams \n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size, layers):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(layer[0], layer[1]) for layer in layers\n",
    "        ])\n",
    "        self.output = nn.Linear(layers[-1][1], 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        #implicit\n",
    "        self.criterion = nn.BCELoss()\n",
    "        #explicit\n",
    "        #self.criterion = nn.MSELoss()\n",
    "        self.optimiser = torch.optim.Adam(self.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "    def forward(self, user_id, item_id):\n",
    "        '''this takes in the available items as a vector of 1-n_of_streams\n",
    "        it multiplies the '''\n",
    "        user_vec = self.user_embedding(user_id)\n",
    "        item_vec = self.item_embedding(item_id)\n",
    "        vector = torch.cat([user_vec, item_vec], dim=-1)\n",
    "        for layer in self.fc_layers:\n",
    "            vector = self.relu(layer(vector))\n",
    "        prediction = torch.sigmoid(self.output(vector))\n",
    "        return prediction\n",
    "    \n",
    "    def train(self, num_epochs, data):\n",
    "        train_loader = \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'epoch {epoch} started\\n')\n",
    "            for user_in, item_in, labels in train_loader:\n",
    "                #send all to gpu\n",
    "                predictions = self.forward(user_in, item_in, labels)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "                self.optimiser.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "        \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
