{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning implementation for CF\n",
    "as shown in this paper \n",
    "\n",
    "J. Rappaz, J. McAuley, and K. Aberer, \"Recommendation on Live-Streaming Platforms: Dynamic Availability and Repeat Consumption,\" EPFL, Lausanne, Switzerland, and University of California, San Diego, CA, USA, 2021. [Online]. Available: https://cseweb.ucsd.edu/~jmcauley/pdfs/recsys21b.pdf. [Accessed: Dec. 23, 2023].\n",
    "\n",
    "we can see that although the SVD approach can recommend streamers to the user it cannot recommend streams due to their reliance on timing as a user cannot consume a live stream before or after it's release. we will use deep learning to try and enhance our recommendations by including the timing of streams and by making sure there is a time variable for when a user logs in and consumes items. For this model our items will be changed to streams in order to make this possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will try and advance the model with\n",
    "\n",
    "\n",
    "streamers are not constantly broadcasting so at the time a user is logged in they are exposed to only items they can view at that time. This means that our positive interactions stay positive however if an item is not avaialable when the user logs in that should not be made a negative interaction. [1] a model should only learn from valid options that are available. \n",
    "\n",
    "as we are modelling temporal data a flat architecture such as an MLP is unsuitable as it will flatten the input and lose any sequencing of the data. we are going to treat the problem as a sequential problem here we will use an attention model similar to the SASRec model shown in this paper:\n",
    "\n",
    "W.-C. Kang and J. McAuley, \"Self-Attentive Sequential Recommendation,\" arXiv, 2018. [Online]. Available: https://arxiv.org/pdf/1808.09781.pdf. [Accessed: Dec. 23, 2023].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating our model inputs\n",
    "the attention model requires sequences of user interactions ordered by time. These sequencces need to all be of the same length this can be achieved by padding with null values if the sequence is too short and removing older items if the sequence is too long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/curranmaguire/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users:  100000\n",
      "Num streamers:  162625\n",
      "Num interactions:  3051733\n"
     ]
    }
   ],
   "source": [
    "# importing the data\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "file_path = os.path.join(os.getcwd(),'Datasets/100k_a.csv')\n",
    "cols = [\"user\",\"stream\",\"streamer\",\"start\",\"stop\"]\n",
    "data = pd.read_csv(file_path, header=None, names=cols)\n",
    "data.user = pd.factorize(data.user)[0]+1\n",
    "data['streamer_raw'] = data.streamer\n",
    "data.streamer = pd.factorize(data.streamer)[0]+1\n",
    "data[['start', 'stop']] = MinMaxScaler().fit_transform(data[['start', 'stop']])\n",
    "print(\"Num users: \", data.user.nunique())\n",
    "print(\"Num streamers: \", data.streamer.nunique())\n",
    "print(\"Num interactions: \", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to show graphs that explain why the timing of my data matters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initial model\n",
    "\n",
    "let's first create an LSTM recommender that will help us recommend items by taking into account the time of consuption of items. LSTM's are good for sequential data as they are a type of RNN. \n",
    "\n",
    "### data preperation\n",
    "first we will need to generate sequences of user input these will need to be orgonized by the time of interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequences(df, sequence_length):\n",
    "    df_sorted = df.sort_values(by=['user', 'start'])\n",
    "    grouped = df_sorted.groupby('user')\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    seq_time = []\n",
    "    for user, group in grouped:\n",
    "        interactions = [x for x in group['stream'].values]\n",
    "        #interactions_time = [tuple(x) for x in group[['stream','start']].values]#allows us to create sequences that can takeinto account the start time for availability\n",
    "        seq = interactions[-sequence_length:-1]\n",
    "        if len(seq) < sequence_length:\n",
    "            for i in range(sequence_length- len(seq)):\n",
    "                seq.insert(0,0)\n",
    "        if len(seq) > sequence_length:\n",
    "            for i in range(len(seq) - sequence_length):\n",
    "                seq.pop(0)\n",
    "        labels.append(interactions[-1])#data target as last item in the list\n",
    "        sequences.append(seq)\n",
    "        return sequences, labels\n",
    "    \n",
    "\n",
    "X,y = generate_sequences(data, 10)\n",
    "np.save('Datasets/X.data', X)\n",
    "np.save('Datasets/y.data', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the LSTM class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class LSTM_model:\n",
    "    def __init__(self, n_features, units, dropout_rate, recurrent_dropout, series_len, prediction_len):\n",
    "        self.n_in = series_len\n",
    "        self.n_out = prediction_len\n",
    "        self.n_features = n_features\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.model = self._build_model()\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(self.units, activation='relu', input_shape=(self.n_in, self.n_features),\n",
    "                       dropout=self.dropout_rate, recurrent_dropout=self.recurrent_dropout))\n",
    "        model.add(RepeatVector(self.n_out))\n",
    "        model.add(LSTM(self.units, activation='relu', return_sequences=True, \n",
    "                       dropout=self.dropout_rate, recurrent_dropout=self.recurrent_dropout))\n",
    "        model.add(TimeDistributed(Dense(self.n_features)))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def train(self, X,y, batch_size, epochs, validation_split = 0.3):\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5, restore_best_weights=True)\n",
    "        self.model.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=validation_split, callbacks=[es])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
