{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project plan\n",
    "\n",
    "The recommender will use collaborative filtering as the dataset only has user -> streamer interactions this means we can match users to streams by getting similar interactions such as when users start watching, how long they watch and who they watch. \n",
    "\n",
    "we can then use more comlplex techniques to model the users returning to streams and to model trends in user interaction.\n",
    "\n",
    "on this paper https://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf of the algorithms discussed we have Biased SGD was one of the best algorithms but alternating Least squares worked best for a very sparse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import surprise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users:  100000\n",
      "Num streamers:  162625\n",
      "Num interactions:  3051733\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(os.getcwd(),'Datasets/100k_a.csv')\n",
    "cols = [\"user\",\"stream\",\"streamer\",\"start\",\"stop\"]\n",
    "data = pd.read_csv(file_path, header=None, names=cols)\n",
    "data.user = pd.factorize(data.user)[0]+1\n",
    "data['streamer_raw'] = data.streamer\n",
    "data.streamer = pd.factorize(data.streamer)[0]+1\n",
    "print(\"Num users: \", data.user.nunique())\n",
    "print(\"Num streamers: \", data.streamer.nunique())\n",
    "print(\"Num interactions: \", len(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preperation \n",
    "\n",
    "make sure the data is 100k in size, make sure any preperation steps are **explained** and **justified**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_train_test_split(interactions_matrix, test_size=0.2):\n",
    "    # Set initial training and testing matrices\n",
    "    train_matrix = np.zeros_like(interactions_matrix)\n",
    "    test_matrix = np.zeros_like(interactions_matrix)\n",
    "    \n",
    "    # Ensure at least one interaction per user and per streamer in the training set\n",
    "    for user in range(interactions_matrix.shape[0]):\n",
    "        user_interactions = np.where(interactions_matrix[user, :] > 0)[0]\n",
    "        if len(user_interactions) > 0:\n",
    "            selected = np.random.choice(user_interactions, size=1)\n",
    "            train_matrix[user, selected] = interactions_matrix[user, selected]\n",
    "\n",
    "    for streamer in range(interactions_matrix.shape[1]):\n",
    "        streamer_interactions = np.where(interactions_matrix[:, streamer] > 0)[0]\n",
    "        if len(streamer_interactions) > 0:\n",
    "            selected = np.random.choice(streamer_interactions, size=1)\n",
    "            train_matrix[selected, streamer] = interactions_matrix[selected, streamer]\n",
    "\n",
    "    # Distribute the rest of the interactions\n",
    "    remaining_indices = np.where((interactions_matrix != train_matrix) & (interactions_matrix > 0))\n",
    "    remaining_indices = list(zip(remaining_indices[0], remaining_indices[1]))\n",
    "    test_indices = np.random.choice(np.arange(len(remaining_indices)), size=int(len(remaining_indices) * test_size), replace=False)\n",
    "    test_indices = [remaining_indices[i] for i in test_indices]\n",
    "\n",
    "    for user, streamer in test_indices:\n",
    "        test_matrix[user, streamer] = interactions_matrix[user, streamer]\n",
    "        train_matrix[user, streamer] = 0\n",
    "    return train_matrix, test_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users:  1474\n",
      "Num streamers:  1380\n",
      "Num interactions:  100000\n",
      "sparsity is  95.08%\n"
     ]
    }
   ],
   "source": [
    "#let's train on the top 100,000 entries. These are the entries which decrease the sparsity the most.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#calculate the user incteraction frequency\n",
    "user_interactions = data.groupby('user').size()\n",
    "streamer_interactions = data.groupby('streamer').size()\n",
    "#rank the users and streamers by their frequency\n",
    "user_ranks = user_interactions.rank(method='first', ascending=False)\n",
    "streamer_ranks = streamer_interactions.rank(method='first', ascending=False)\n",
    "#score the interactions and select the top 100,000\n",
    "data['user_score'] = data['user'].map(user_ranks)\n",
    "data['streamer_score'] = data['streamer'].map(streamer_ranks)\n",
    "data['combined_score'] = data['user_score'] + data['streamer_score']\n",
    "data['rating'] = data.stop - data.start\n",
    "\n",
    "\n",
    "top_data = data.nsmallest(100000, 'combined_score')\n",
    "filtered_data = top_data.drop(columns=['user_score', 'streamer_score', 'combined_score'])\n",
    "\n",
    "print(\"Num users: \", filtered_data.user.nunique())\n",
    "print(\"Num streamers: \", filtered_data.streamer.nunique())\n",
    "print(\"Num interactions: \", len(filtered_data))\n",
    "\n",
    "# check for sparsity of the data\n",
    "potential_num_interactions = filtered_data.streamer.nunique() * filtered_data.user.nunique()\n",
    "num_interactions = len(filtered_data['streamer']) #each row in the filtered_data is an interaction\n",
    "sparsity = (1-num_interactions /potential_num_interactions) * 100\n",
    "print(f'sparsity is {sparsity: .2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the user - streamer matrix for SVD\n",
    "\n",
    "make a matrix of users - streams this will then be used to push streamers to the user as a stream is pushed a streamer is the selected by this stream. \n",
    "\n",
    "**potentially introduce bias to streamers that post a lot of streams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1474, 1380) (1474, 1380) (1474, 1380)\n"
     ]
    }
   ],
   "source": [
    "user_streamer_interactions = filtered_data.pivot_table(index='user', columns='streamer', values='rating', aggfunc='sum', fill_value=0).values\n",
    "#normalize the items \n",
    "user_streamer_interactions = MinMaxScaler().fit_transform(user_streamer_interactions)\n",
    "\n",
    "#this makes sure that all the users and streamers are represented in the training and testing data\n",
    "train_interactions, test_interactions = custom_train_test_split(user_streamer_interactions, test_size=0.2)\n",
    "\n",
    "print(user_streamer_interactions.shape, test_interactions.shape, train_interactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there overlap in the train and test sets? False\n"
     ]
    }
   ],
   "source": [
    "def check_overlap(train_matrix, test_matrix):\n",
    "    # Check if any element is present in both matrices\n",
    "    overlap = np.sum((train_matrix > 0) & (test_matrix > 0))\n",
    "    return overlap > 0\n",
    "\n",
    "overlap_exists = check_overlap(train_interactions, test_interactions)\n",
    "print(\"Is there overlap in the train and test sets?\", overlap_exists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD implementation\n",
    "we will train the model using biased stochiastic gradient descent \n",
    "\n",
    "C. Aberger, \"Recommender,\" Project Report, CS229, Stanford University, 2014. [Online]. Available: https://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf. [Accessed: Dec. 23, 2023].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD using BSGD class creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basic_recommender:\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def predict_single(self, user,streamer):\n",
    "        return (self.global_bias + self.user_biases[user] + self.streamer_biases[streamer] + self.user_features[user,:].dot(self.streamer_features[streamer,:]))\n",
    "   \n",
    "    \n",
    "    def train(self, epochs, n_features, learning_rate, regularisation_strength):\n",
    "        n_users = self.data.shape[0]\n",
    "        n_streamers = self.data.shape[1]\n",
    "        self.user_features = np.random.normal(0,0.1, (n_users, n_features))\n",
    "        self.streamer_features = np.random.normal(0,0.1, (n_streamers, n_features))\n",
    "        self.user_biases = np.zeros(n_users)\n",
    "        self.streamer_biases = np.zeros(n_streamers)\n",
    "        self.global_bias = np.mean(self.data[self.data != 0])\n",
    "\n",
    "\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for user in range(n_users):\n",
    "                for streamer in range(n_streamers):\n",
    "                    interaction = self.data[user, streamer]\n",
    "                    if interaction > 0: #makes sure you skip values that are non interactions\n",
    "                        prediction = self.predict_single(user, streamer)\n",
    "                        \n",
    "                        error = interaction - prediction\n",
    "\n",
    "                        self.user_features[user,:] += learning_rate * (error * self.streamer_features[streamer,:] - regularisation_strength * self.user_features[user, :])\n",
    "                        self.streamer_features[streamer,:] += learning_rate * (error * self.user_features[user,:] - regularisation_strength * self.streamer_features[streamer, :])\n",
    "                        self.user_biases[user] += learning_rate * (error - regularisation_strength*self.user_biases[user])\n",
    "                        self.streamer_biases[streamer] += learning_rate * (error - regularisation_strength*self.streamer_biases[streamer])\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        predictions = np.zeros(dataset.shape)\n",
    "        for user in range(dataset.shape[0]):\n",
    "            for streamer in range(dataset.shape[1]):\n",
    "                if dataset[user, streamer] > 0:\n",
    "                    predictions[user, streamer] = self.predict_single(user,streamer)\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, dataset):\n",
    "        predictions = self.predict(dataset)\n",
    "        predictions = predictions[dataset != 0].flatten()\n",
    "        actual = dataset[dataset != 0].flatten()\n",
    "\n",
    "        return np.sqrt(mean_squared_error(actual, predictions))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model evaluation \n",
    "first we will check the model against the surprise SVD as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "user_ids, item_ids = np.where(train_interactions > 0)  # Get user, item indices for non-zero interactions\n",
    "ratings = train_interactions[user_ids, item_ids]  # Extract corresponding ratings\n",
    "\n",
    "# Create a DataFrame from train_interactions\n",
    "train_df = pd.DataFrame({\n",
    "    'userID': user_ids,\n",
    "    'itemID': item_ids,\n",
    "    'rating': ratings\n",
    "})\n",
    "# Assuming test_interactions is your test data in matrix form\n",
    "user_ids, item_ids = np.where(test_interactions > 0)  # Get user, item indices for non-zero interactions\n",
    "ratings = test_interactions[user_ids, item_ids]  # Extract corresponding ratings\n",
    "\n",
    "# Create the testset as a list of tuples\n",
    "testset = list(zip(user_ids, item_ids, ratings))\n",
    "\n",
    "\n",
    "# Load the train set into Surprise\n",
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "train_data = Dataset.load_from_df(train_df[['userID', 'itemID', 'rating']], reader)\n",
    "trainset = train_data.build_full_trainset()  # Build the trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Splitting the dataset into train and test set\n",
    "\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "# Example hyperparameters\n",
    "n_features = 100  # Number of latent factors\n",
    "n_epochs = 20    # Number of epochs\n",
    "lr_all = 0.005   # Learning rate\n",
    "reg_all = 0.02   # Regularization term\n",
    "\n",
    "algo = SVD(n_factors=n_features, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculate RMSE\n",
    "surprise_rmse = rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2777\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVDpp\n",
    "algo = SVDpp(n_factors=n_features, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all)\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculate RMSE\n",
    "surprise_rmse = rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2569604620767916\n"
     ]
    }
   ],
   "source": [
    "SVD = basic_recommender(train_interactions)\n",
    "SVD.train(\n",
    "    epochs=n_epochs,\n",
    "    n_features=n_features,\n",
    "    learning_rate=lr_all,\n",
    "    regularisation_strength=reg_all,\n",
    ")\n",
    "print(SVD.evaluate(test_interactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model saving and use in interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as my model works the best we will use it in the app. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
